{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UFSCAT-Chem Fire Emission Model Demo\n",
    "\n",
    "This notebook demonstrates the full pipeline for the `UFSCATChemFireGenerator` model, including:\n",
    "1. Initialization\n",
    "2. Loading and generating mock data\n",
    "3. Running the training process\n",
    "4. Exporting the final LUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from sofiev_model.ufscat_fire_generator import UFSCATChemFireGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_sys = UFSCATChemFireGenerator(target_res=0.5) # Use a coarse resolution for demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Mock Data\n",
    "\n",
    "In a real scenario, you would load your `gbbepx` and `ufs` meteorology datasets here. For this demo, we'll create synthetic data that matches the expected structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mock_satellite_data(lats, lons, time_steps=36):\n",
    "    \"\"\"Creates a mock 500m satellite dataset.\"\"\"\n",
    "    coords = {'lat': lats, 'lon': lons, 'time': pd.to_datetime(pd.date_range('2022-01-01', periods=time_steps, freq='M'))}\n",
    "    \n",
    "    # Make FRP and LAI vary with location and time\n",
    "    lat_factor = np.abs(np.cos(np.deg2rad(lats))).reshape(-1, 1)\n",
    "    lon_factor = (np.sin(np.deg2rad(lons)) + 1).reshape(1, -1)\n",
    "    time_factor = np.sin(np.arange(time_steps) / 12 * np.pi).reshape(1, 1, -1) # Seasonal cycle\n",
    "\n",
    "    frp_data = 100 * lat_factor * lon_factor * time_factor * np.random.rand(len(lats), len(lons), time_steps)\n",
    "    lai_data = 5 * lat_factor * time_factor\n",
    "    igbp_data = np.random.randint(1, 17, size=(len(lats), len(lons)))\n",
    "    \n",
    "    return xr.Dataset({\n",
    "        'FRP': (['lat', 'lon', 'time'], frp_data),\n",
    "        'LAI': (['lat', 'lon', 'time'], np.broadcast_to(lai_data[:, :, np.newaxis], frp_data.shape)),\n",
    "        'IGBP': (['lat', 'lon'], igbp_data)\n",
    "    }, coords=coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mock_met_data(lats, lons, time_steps=36):\n",
    "    \"\"\"Creates a mock 4km meteorology dataset.\"\"\"\n",
    "    coords = {'lat': lats, 'lon': lons, 'time': pd.to_datetime(pd.date_range('2022-01-01', periods=time_steps, freq='M'))}\n",
    "    \n",
    "    vpd_data = np.random.uniform(0, 30, size=(len(lats), len(lons), time_steps))\n",
    "    soil_m_data = np.random.uniform(0.1, 0.5, size=(len(lats), len(lons), time_steps))\n",
    "\n",
    "    return xr.Dataset({\n",
    "        'vpd': (['lat', 'lon', 'time'], vpd_data),\n",
    "        'soil_m': (['lat', 'lon', 'time'], soil_m_data)\n",
    "    }, coords=coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Using very coarse resolution for speed\n",
    "mock_lats_500m = np.arange(-90, 90, 2.0)\n",
    "mock_lons_500m = np.arange(-180, 180, 2.0)\n",
    "ds_500m = create_mock_satellite_data(mock_lats_500m, mock_lons_500m)\n",
    "\n",
    "# The met data should be at the target resolution\n",
    "met_4km = create_mock_met_data(fire_sys.target_lats, fire_sys.target_lons)\n",
    "\n",
    "print(\"Mock 500m Satellite Data:\")\n",
    "print(ds_500m)\n",
    "print(\"\\nMock 4km Meteorology Data:\")\n",
    "print(met_4km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Aggregate raw data\n",
    "# We need to aggregate each time step individually. We can do this with a list comprehension.\n",
    "ds_4km_slices = [fire_sys.aggregate_raw_data(ds_500m.isel(time=t)) for t in range(len(ds_500m.time))]\n",
    "\n",
    "# Concatenate the slices along the time dimension\n",
    "ds_4km_time = xr.concat(ds_4km_slices, dim=ds_500m.time)\n",
    "\n",
    "print(\"Aggregated 4km Data over Time:\")\n",
    "print(ds_4km_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate features\n",
    "training_df = fire_sys.generate_features(ds_4km_time, met_4km)\n",
    "\n",
    "print(\"\\nGenerated Training DataFrame:\")\n",
    "print(training_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train the XGBoost model\n",
    "xgb_model = fire_sys.train_xgboost(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export the LUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Export the binary LUT for Fortran\n",
    "fire_sys.export_binary_lut(xgb_model, filename=\"fire_scaling_lut_demo.bin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
